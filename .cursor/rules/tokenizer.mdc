---
description: Tutorial chapter for the jaxgarden Tokenizer, detailing text encoding/decoding and chat templating for JAX.
globs: 
alwaysApply: false
---
# Chapter 1: Tokenizer

Welcome to the `jaxgarden` library tutorial! This first chapter introduces the `Tokenizer` class, a fundamental component for processing text data within JAX-based Natural Language Processing (NLP) models.

**Motivation:** Deep learning models, especially those built with JAX, operate on numerical tensors. Raw text needs to be converted into a numerical format (token IDs) that models can understand, and conversely, model outputs (token IDs) need to be converted back into human-readable text. Furthermore, different models, particularly instruction-tuned ones, expect conversational inputs to be formatted in specific ways (chat templates). The Hugging Face `tokenizers` library is excellent for this, but its outputs are standard Python lists. `jaxgarden.Tokenizer` wraps this library to provide a seamless experience for JAX users, returning `jax.numpy.ndarray` (jnp arrays) directly and integrating features like chat templating.

**Central Use Case:** Preparing text input for a JAX-based language model like [LlamaForCausalLM](llamaforcausallm.mdc) and decoding its generated token IDs. For conversational models, formatting user prompts and conversation history according to the model's specific chat template is crucial.

## Key Concepts

The `jaxgarden.Tokenizer` provides several core functionalities:

1.  **Loading:** Instantiating a tokenizer from pre-trained configurations stored on the Hugging Face Hub or locally.
2.  **Encoding:** Converting text strings into sequences of token IDs, handling padding and truncation, and returning JAX arrays.
3.  **Decoding:** Converting sequences of token IDs back into text strings.
4.  **Special Token Management:** Automatically identifying or allowing specification of crucial tokens like Beginning-of-Sequence (BOS), End-of-Sequence (EOS), and Padding (PAD).
5.  **Chat Templating:** Applying Jinja-based templates to format conversational data for instruction-tuned models.

## Using the Tokenizer

Let's explore how to use the `Tokenizer`.

### Loading a Tokenizer

The primary way to get a `Tokenizer` instance is using the `from_pretrained` class method. You provide a model identifier from the Hugging Face Hub (e.g., `"gpt2"`, `"meta-llama/Llama-2-7b-chat-hf"`) or a path to a local directory containing `tokenizer.json` and optionally `tokenizer_config.json`.

```python
# Assuming jaxgarden is installed
from jaxgarden.tokenization import Tokenizer

# Load from Hugging Face Hub
tokenizer = Tokenizer.from_pretrained("gpt2")

# Example: Load from a local directory (if you have one)
# tokenizer_local = Tokenizer.from_pretrained("./path/to/local_tokenizer_files")

print(f"Loaded tokenizer for 'gpt2' with vocab size: {tokenizer.vocab_size}")
print(f"Pad token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}")
print(f"BOS token: {tokenizer.bos_token}, ID: {tokenizer.bos_token_id}")
print(f"EOS token: {tokenizer.eos_token}, ID: {tokenizer.eos_token_id}")
```

**Explanation:** `from_pretrained` downloads necessary files (`tokenizer.json`, `tokenizer_config.json`) from the Hub or reads them locally. It then instantiates the underlying Hugging Face `tokenizers.Tokenizer` and extracts configuration like special tokens and chat templates (if available in `tokenizer_config.json`). The `jaxgarden.Tokenizer` wrapper uses this information to set its own attributes like `pad_token_id`, `bos_token_id`, etc.

### Encoding Text

The `encode` method converts text into token IDs. It offers options for handling batches, padding, and truncation, returning JAX arrays by default.

```python
import jax.numpy as jnp

text = "Hello, world!"
batch_text = ["First sequence.", "This is a second sequence."]

# Basic encoding
encoded_single = tokenizer.encode(text)
print("Encoded Single:", encoded_single)
# Output: Encoded Single: {'input_ids': DeviceArray([[50256, 15496,  11,  1917,   25, 50256]], dtype=int32),
#                       'attention_mask': DeviceArray([[1, 1, 1, 1, 1, 1]], dtype=int32)}

# Encoding a batch with padding to the longest sequence
encoded_batch = tokenizer.encode(batch_text, padding=True, add_special_tokens=False)
print("Encoded Batch (padded):", encoded_batch)
# Output: Encoded Batch (padded): {
#  'input_ids': DeviceArray([[ 8285, 16337,    13, 50256, 50256, 50256],
#                          [ 1212,   318,   257,  1144, 16337,    13]], dtype=int32),
#  'attention_mask': DeviceArray([[1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1]], dtype=int32) }


# Encoding with truncation and padding to a max length
encoded_truncated = tokenizer.encode(
    batch_text, padding="max_length", truncation=True, max_length=5, add_special_tokens=False
)
print("Encoded Batch (truncated/padded):", encoded_truncated)
# Output: Encoded Batch (truncated/padded): {
# 'input_ids': DeviceArray([[ 8285, 16337,    13, 50256, 50256],
#                         [ 1212,   318,   257,  1144, 16337]], dtype=int32),
# 'attention_mask': DeviceArray([[1, 1, 1, 0, 0], [1, 1, 1, 1, 1]], dtype=int32)}

```

**Explanation:**
- `text`: The input string or list of strings.
- `add_special_tokens`: Controls whether BOS/EOS tokens are added (based on tokenizer config). Default is `True`.
- `padding`: Can be `False` (no padding), `True` or `'longest'` (pad to the longest sequence in the batch), or `'max_length'` (pad to `max_length`). Requires `max_length` if set to `'max_length'`.
- `truncation`: Boolean. If `True`, truncates sequences longer than `max_length`. Requires `max_length`.
- `max_length`: Integer specifying the target length for padding or truncation.
- `return_tensors`: Set to `"jax"` to get `jnp.ndarray`. Set to `None` to get lists of integers.
The output is a dictionary containing `input_ids` and `attention_mask` as JAX arrays. The attention mask indicates which tokens are real (1) and which are padding (0).

### Decoding Token IDs

The `decode` method converts token IDs back into human-readable text.

```python
# Use the 'input_ids' from the previous encoding example
ids_to_decode = encoded_batch['input_ids'] # Example: DeviceArray([[ 8285, 16337,    13, 50256, 50256, 50256], ...])

# Decode the batch
decoded_text = tokenizer.decode(ids_to_decode, skip_special_tokens=True)
print("Decoded Text:", decoded_text)
# Output: Decoded Text: ['First sequence.', 'This is a second sequence.']

# Decode a single sequence (e.g., the first one from the batch)
single_decoded = tokenizer.decode(ids_to_decode[0], skip_special_tokens=True)
print("Single Decoded:", single_decoded)
# Output: Single Decoded: First sequence.
```

**Explanation:**
- `token_ids`: A list of integers, a list of lists of integers, or a JAX array.
- `skip_special_tokens`: If `True` (default), removes special tokens like BOS, EOS, PAD from the output string(s).

### Applying Chat Templates

For models fine-tuned on conversational data, inputs must be formatted correctly. The `apply_chat_template` method uses a Jinja template (either defined in the tokenizer's config or provided explicitly) to structure conversations.

```python
# Load a tokenizer known to have a chat template
# Note: You might need to log in to Hugging Face CLI: `huggingface-cli login`
try:
    chat_tokenizer = Tokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf") # Example
except Exception as e:
    print(f"Skipping chat template example: Could not load Llama-2 tokenizer. Error: {e}")
    chat_tokenizer = None # Set to None to skip the rest of the block

if chat_tokenizer and chat_tokenizer.chat_template:
    conversation = [
        {"role": "user", "content": "Hello, how are you?"},
        {"role": "assistant", "content": "I'm doing well, thank you!"},
        {"role": "user", "content": "What is JAX?"}
    ]

    # Apply the template to format the conversation
    formatted_prompt = chat_tokenizer.apply_chat_template(
        conversation,
        add_generation_prompt=True # Adds the prompt for the assistant's turn
    )
    print("\nFormatted Chat Prompt:\n", formatted_prompt)

    # Example of Llama-2 Chat format (output structure may vary slightly):
    # Formatted Chat Prompt:
    #  <s>[INST] Hello, how are you? [/INST] I'm doing well, thank you! </s><s>[INST] What is JAX? [/INST]
else:
      print("\nSkipping chat template example: No chat_tokenizer or template found.")

```

**Explanation:**
- `conversation`: A list of dictionaries, each with a `role` (e.g., 'user', 'assistant', 'system') and `content` (the message text).
- `chat_template`: Optional override for the tokenizer's default template.
- `add_generation_prompt`: A common pattern in chat templates is to append tokens indicating it's the assistant's turn to speak. Setting this to `True` often achieves this, though the exact implementation depends on the template itself.
- `**kwargs`: Allows passing additional variables to the Jinja template.
The method returns the formatted string, ready to be encoded and fed into the model.

## Internal Implementation

Understanding the internals helps in debugging and extending functionality.

1.  **Initialization (`__init__`)**:
    - Stores the Hugging Face `HfTokenizer` instance.
    - Tries to infer `bos_token`, `eos_token`, and `pad_token` from the `HfTokenizer`'s known special tokens or common defaults (e.g., `<s>`, `</s>`, `[PAD]`). It logs warnings if defaults or fallbacks are used.
    - If `pad_token` isn't explicitly provided or found, it defaults to `eos_token` (with a warning) or raises an error if no suitable token can be determined.
    - It fetches the corresponding IDs (`bos_token_id`, etc.) using `hf_tokenizer.token_to_id`.
    - Critically, it ensures the `HfTokenizer` has padding configured using the determined `pad_token_id` and `pad_token`, either by enabling it or verifying/correcting existing padding settings.

    ```python
    # Simplified __init__ logic for special tokens
    def __init__(self, hf_tokenizer, chat_template=None, bos_token=None, /*...*/ pad_token=None):
        self.hf_tokenizer = hf_tokenizer
        # ... (chat_template assignment) ...

        # Infer BOS/EOS (simplified example)
        self.bos_token = bos_token or self._infer_token(hf_tokenizer, ["[BOS]", "<s>"])
        self.eos_token = eos_token or self._infer_token(hf_tokenizer, ["[EOS]", "</s>"])

        # Infer/Validate PAD token
        if pad_token:
            self.pad_token = pad_token
        elif hf_tokenizer.padding and hf_tokenizer.padding.get("pad_token"):
            self.pad_token = hf_tokenizer.padding["pad_token"]
        # ... (fallback logic using EOS or searching for [PAD]/<pad>) ...
        else:
            raise ValueError("Cannot determine padding token.")

        # Get IDs
        self.bos_token_id = hf_tokenizer.token_to_id(self.bos_token) # if self.bos_token else None
        self.eos_token_id = hf_tokenizer.token_to_id(self.eos_token) # if self.eos_token else None
        self.pad_token_id = hf_tokenizer.token_to_id(self.pad_token)
        # ... (error if pad_token_id is None) ...

        # Configure HF tokenizer padding
        self.hf_tokenizer.enable_padding(pad_id=self.pad_token_id, pad_token=self.pad_token)
        # ... (logic to handle pre-existing padding config) ...

    # Helper function (conceptual)
    def _infer_token(self, hf_tok, candidates):
        for token_str in candidates:
            if hf_tok.token_to_id(token_str) is not None:
                return token_str
        return None
    ```

2.  **Loading (`from_pretrained`)**:
    - Checks if the `identifier` is a local path.
    - If local: Looks for `tokenizer.json` and `tokenizer_config.json` in that directory.
    - If not local (Hub): Uses `huggingface_hub.hf_hub_download` to fetch the files.
    - Loads the core tokenizer from `tokenizer.json` using `HfTokenizer.from_file`.
    - Loads the configuration (chat template, special tokens) from `tokenizer_config.json` if it exists.
    - Prioritizes explicitly passed arguments (`bos_token`, `eos_token`, etc.) over values found in `tokenizer_config.json`.
    - Calls the `__init__` method with the loaded `HfTokenizer` and extracted/provided configuration.

3.  **Encoding (`encode`)**:
    - Temporarily configures the `hf_tokenizer` instance based on `padding`, `truncation`, and `max_length` arguments by calling `hf_tokenizer.enable_padding`, `hf_tokenizer.enable_truncation`, or `hf_tokenizer.no_padding`/`no_truncation`.
    - Configures the `hf_tokenizer.post_processor` using `TemplateProcessing` if `add_special_tokens=True` and BOS/EOS tokens are available, otherwise sets it to `None`. This handles adding BOS/EOS correctly.
    - Calls `hf_tokenizer.encode` (for single text) or `hf_tokenizer.encode_batch` (for list of texts).
    - Extracts the `ids` and `attention_mask` from the result(s).
    - If `return_tensors="jax"`, converts the lists of integers into `jnp.int32` arrays using `jnp.array`. It handles potential raggedness after encoding if `padding='longest'` was used by manually padding again to the true max length found in the batch *after* encoding.
    - Returns the dictionary `{'input_ids': ..., 'attention_mask': ...}`.

    ```mermaid
    sequenceDiagram
        participant User
        participant Tokenizer as JaxTokenizer
        participant HfTokenizer as HF Tokenizer
        participant JNP as jax.numpy

        User->>+JaxTokenizer: encode(text, padding=True, ...)
        JaxTokenizer->>+HfTokenizer: enable_padding(pad_id, pad_token, ...)
        JaxTokenizer->>+HfTokenizer: enable_truncation(...) / no_truncation() (based on args)
        JaxTokenizer->>+HfTokenizer: set post_processor (for special tokens)
        JaxTokenizer->>+HfTokenizer: encode_batch(text) / encode(text)
        HfTokenizer-->>-JaxTokenizer: list[Encoding] (contains ids, attention_mask)
        JaxTokenizer->>JaxTokenizer: Extract ids and masks into lists
        alt return_tensors == "jax"
            JaxTokenizer->>JNP: array(ids_list, dtype=int32)
            JNP-->>JaxTokenizer: ids_array (jnp.ndarray)
            JaxTokenizer->>JNP: array(mask_list, dtype=int32)
            JNP-->>JaxTokenizer: mask_array (jnp.ndarray)
            JaxTokenizer-->>-User: {'input_ids': ids_array, 'attention_mask': mask_array}
        else
            JaxTokenizer-->>-User: {'input_ids': ids_list, 'attention_mask': mask_list}
        end

    ```

4.  **Decoding (`decode`)**:
    - Converts input `jnp.ndarray` to lists if necessary.
    - Determines if the input is a single sequence or a batch.
    - Calls the underlying `hf_tokenizer.decode` or `hf_tokenizer.decode_batch` method, passing `skip_special_tokens`.
    - Returns the resulting string or list of strings.

5.  **Chat Templating (`apply_chat_template`)**:
    - Selects the Jinja template (either `self.chat_template` or the one passed as an argument). Raises an error if none is available.
    - Creates a dictionary of variables to pass to the template, including `messages`, `bos_token`, `eos_token`, and any `**kwargs`.
    - Performs basic validation on the template structure (optional, checks for expected variables like `messages`).
    - Modifies the template string if `add_generation_prompt=True` (using a common but potentially model-specific pattern).
    - Creates a Jinja environment and renders the template with the variables.
    - Returns the rendered string.

## Conclusion

The `jaxgarden.Tokenizer` provides a crucial bridge between raw text and JAX-based NLP models. It leverages the power of Hugging Face `tokenizers` while ensuring compatibility with JAX workflows by returning `jnp.ndarray` objects. Key functionalities include easy loading from the Hub/local files, robust encoding/decoding with padding and truncation control, automatic handling of special tokens, and essential chat templating for conversational AI.

Understanding how to use the `Tokenizer` is the first step in building or using models within `jaxgarden`. The next chapter will introduce the foundational building blocks for models themselves.

**Next:** [BaseModel](basemodel.mdc)


---

Generated by [Rules for AI](https://github.com/altaidevorg/rules-for-ai)